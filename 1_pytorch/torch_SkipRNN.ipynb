{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0218c5c8-8388-4bda-8b87-2c9d8e387a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbh/.conda/envs/dnn/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable, Function\n",
    "\n",
    "class BinaryLayer(Function):\n",
    "    def forward(self, input):\n",
    "        return input.round()\n",
    " \n",
    "    def backward(self, grad_output):\n",
    "        return grad_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7566c8-01a7-4c91-9586-36e7e52facb5",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/implementation-of-multiplicative-lstm/2328/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e69a07-d94c-407e-b3e9-7e4c2d35eaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipRNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,num_layers=1,bias=True,dropout=0,batch_first=True,bidrectional=False):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.ir = nn.Linear(input_size,hidden_size,bias=bias)\n",
    "        self.iu = nn.Linear(input_size,hidden_size,bias=bias)\n",
    "        self.in = nn.Linear(input_size,hidden_size,bias=bias)\n",
    "        \n",
    "        self.hr = nn.Linear(input_size,hidden_size,bias=bias)\n",
    "        self.hu = nn.Linear(input_size,hidden_size,bias=bias)\n",
    "        self.hn = nn.Linear(input_size,hidden_size,bias=bias)\n",
    "        \n",
    "        if bidirectional :\n",
    "            raise(\"SkipRNN::bidirectional not implemented\")\n",
    "            \n",
    "    def forward(self,x,h=None) :\n",
    "        \n",
    "        if h == None : \n",
    "              h = torch.zeros(1,self.hidden_size).to(x.shape)\n",
    "        \n",
    "        def recurrent(input,hidden):\n",
    "            r = F.sigmoid(self.ir(input) + self.hr(hidden))\n",
    "            u = F.sigmoid(self.iu(input) + self.hu(hidden))\n",
    "            n - F.tanh(self.in(input) + r*self.hn(hiddenn))\n",
    "            hidden = (1-u)*n + u*hidden\n",
    "            \n",
    "            return hidden\n",
    "        \n",
    "        n_step = x.shape[1]\n",
    "        o = []\n",
    "        \n",
    "        for i in range(n_step) : \n",
    "            h = recurrent(x[:,i,:],h)\n",
    "            output.append(hidden)\n",
    "        #o = torch.cat(o, 1).view(x.size(0), *o[0].size())\n",
    "        \n",
    "        return o,h\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5925e44-6561-47cd-8f48-053e47a07832",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b4d432-92a6-4152-a516-b48cd038a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "from torch.nn.init import xavier_normal_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46a91820-9b57-4d76-ab3c-8cebe4239902",
   "metadata": {},
   "outputs": [],
   "source": [
    "class STEFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(cls,x):\n",
    "        return x.round()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(cls,grad):\n",
    "        return grad\n",
    "\n",
    "\n",
    "class STELayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(STELayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        binarizer = STEFunction.apply\n",
    "        return binarizer(x)\n",
    "\n",
    "\n",
    "class SkipGRUCell(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_size):\n",
    "        super(SkipGRUCell, self).__init__()\n",
    "        self.ste = STELayer()\n",
    "        self.cell = nn.GRUCell(in_channels, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        xavier_normal_(self.linear.weight)\n",
    "        self.linear.bias.data.fill_(1)\n",
    "\n",
    "    def forward(self, x, u, h, skip=False, delta_u=None):\n",
    "        # x: (bs, in_channels)\n",
    "        # u: (bs, 1)\n",
    "        # h: (bs, hidden_size)\n",
    "        # skip: [False or True] * bs\n",
    "        # delta_u: [skip=True -> (1) / skip=False -> None] * bs\n",
    "\n",
    "        bs = x.shape[0]\n",
    "        binarized_u = self.ste(u)                # (bs, 1)\n",
    "\n",
    "        skip_idx = [i for i, cur_skip in enumerate(skip) if cur_skip]\n",
    "        skip_num = len(skip_idx)\n",
    "        no_skip = [not cur_skip for cur_skip in skip]\n",
    "\n",
    "        if skip_num > 0:\n",
    "            # (skip_num, in_channels), (skip_num, 1), (skip_num, hidden_size)\n",
    "            x_s, u_s, h_s = x[skip], u[skip], h[skip]\n",
    "            binarized_u_s = binarized_u[skip]        # (skip_num, 1)\n",
    "\n",
    "            # (skip_num, 1)\n",
    "            delta_u_s = [cur_delta_u for cur_skip,\n",
    "                         cur_delta_u in zip(skip, delta_u) if cur_skip]\n",
    "            delta_u_s = torch.stack(delta_u_s)\n",
    "\n",
    "            # computing skipped parts\n",
    "            new_h_s = h_s * (1 - binarized_u_s)        # (skip_num, hidden_size)\n",
    "            new_u_s = torch.clamp(u_s + delta_u_s, 0, 1) * \\\n",
    "                (1 - binarized_u_s)  # (skip_num, 1)\n",
    "\n",
    "        if skip_num < bs:\n",
    "            # (bs-skip_num, in_channels), (bs-skip_num, 1), (bs-skip_num, hidden_size)\n",
    "            x_n, u_n, h_n = x[no_skip], u[no_skip], h[no_skip]\n",
    "            binarized_u_n = binarized_u[no_skip]  # (bs-skip_num, 1)\n",
    "\n",
    "            # computing non-skipped parts\n",
    "            new_h_n = self.cell(x_n, h_n)  # (bs-skip_num, hidden_size)\n",
    "            new_h_n = new_h_n * binarized_u_n            # (bs-skip_num, hidden_size)\n",
    "            delta_u_n = torch.sigmoid(self.linear(new_h_n))        # (bs-skip_num, 1)\n",
    "            new_u_n = delta_u_n * binarized_u_n                    # (bs-skip_num, 1)\n",
    "\n",
    "        # merging skipped and non-skipped parts back\n",
    "        if 0 < skip_num < bs:\n",
    "            idx = torch.full((bs,), -1, dtype=torch.long)\n",
    "            idx[skip_idx] = torch.arange(0, len(skip_idx), dtype=torch.long)\n",
    "            idx[idx==-1] = torch.arange(len(skip_idx), bs, dtype=torch.long)\n",
    "\n",
    "            new_u = torch.cat([new_u_s, new_u_n], 0)[idx]        # (bs, 1)\n",
    "            new_h = torch.cat([new_h_s, new_h_n], 0)[idx]        # (bs, hidden_size)\n",
    "            delta_u = torch.cat([delta_u_s, delta_u_n], 0)[idx]    # (bs, 1)\n",
    "\n",
    "        # no need to merge when skip doesn't exist\n",
    "        elif skip_num == 0:\n",
    "            new_u = new_u_n\n",
    "            new_h = new_h_n\n",
    "            delta_u = delta_u_n\n",
    "\n",
    "        # no need to merge when everything is skip\n",
    "        elif skip_num == bs:\n",
    "            new_u = new_u_s\n",
    "            new_h = new_h_s\n",
    "            delta_u = delta_u_s\n",
    "\n",
    "        n_skips_after = (0.5 / new_u).ceil() - 1  # (bs, 1)\n",
    "        return binarized_u, new_u, (new_h,), delta_u, n_skips_after\n",
    "\n",
    "class SkipGRU(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_size, layer_num=2, return_total_u=False, learn_init=False, batch_first = True):\n",
    "        super(SkipGRU, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layer_num = layer_num\n",
    "        self.return_total_u = return_total_u\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        cur_cell = SkipGRUCell\n",
    "\n",
    "        self.cells = nn.ModuleList([cur_cell(in_channels, hidden_size)])\n",
    "        for _ in range(self.layer_num - 1):\n",
    "            cell = cur_cell(hidden_size, hidden_size)\n",
    "            self.cells.append(cell)\n",
    "\n",
    "        self.hiddens = self.init_hiddens(learn_init)\n",
    "\n",
    "    def init_hiddens(self, learn_init):\n",
    "        if learn_init:\n",
    "            h = nn.Parameter(torch.randn(self.layer_num, 1, self.hidden_size))\n",
    "        else:\n",
    "            h = nn.Parameter(torch.zeros(self.layer_num, 1, self.hidden_size), requires_grad=False)\n",
    "        return h\n",
    "\n",
    "    def forward(self, x, hiddens=None):\n",
    "        device = x.device\n",
    "        if self.batch_first : \n",
    "            x = torch.permute(x,(1,0,2))\n",
    "        \n",
    "        x_len, bs, _ = x.shape    # (x_len, bs, in_channels)\n",
    "\n",
    "        if hiddens is None:\n",
    "            h = self.hiddens\n",
    "        else:\n",
    "            h = hiddens\n",
    "        h = h.repeat(1, bs, 1)\n",
    "        u = torch.ones(self.layer_num, bs, 1).to(device)            # (l, bs, 1)\n",
    "        \n",
    "\n",
    "        hs = []\n",
    "        lstm_input = x             # (x_len, bs, in_channels)\n",
    "\n",
    "        skip = [False] * bs\n",
    "        delta_u = [None] * bs\n",
    "\n",
    "        binarized_us = []\n",
    "\n",
    "        for i in range(self.layer_num):\n",
    "            cur_hs = []\n",
    "            cur_h = h[i].unsqueeze(0)  # (1, bs, hidden_size)\n",
    "            cur_u = u[i]               # (bs, 1)\n",
    "            \n",
    "\n",
    "            for j in range(x_len):\n",
    "                # (bs, 1), ((bs, hidden_size), (bs, hidden_size)), (bs, 1), (bs, 1)\n",
    "                binarized_u, cur_u, cur_h, delta_u, n_skips_after = self.cells[i](\n",
    "                    lstm_input[j], cur_u, cur_h[0], skip, delta_u)\n",
    "                binarized_us.append(binarized_u)\n",
    "                skip = (n_skips_after[:, 0] > 0).tolist()\n",
    "\n",
    "                # (1, bs, hidden_size) / (1, bs, hidden_size)\n",
    "                cur_h = cur_h[0].unsqueeze(0)\n",
    "                cur_hs.append(cur_h)\n",
    "\n",
    "            # (x_len, bs, hidden_size)\n",
    "            lstm_input = torch.cat(cur_hs, dim=0)\n",
    "            hs.append(cur_h)\n",
    "\n",
    "        # (bs, seq * layer_num)\n",
    "        total_u = torch.cat(binarized_us, 1)\n",
    "        # (x_len, bs, hidden_size)\n",
    "        out = lstm_input\n",
    "        # (l, bs, hidden_size)\n",
    "        hs = torch.cat(hs, dim=0)\n",
    "        \n",
    "        if self.batch_first : \n",
    "            out = torch.permute(out,(1,0,2))\n",
    "\n",
    "        if self.return_total_u:\n",
    "            return out, (hs,), total_u\n",
    "\n",
    "        return out, (hs,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b773e34-4ed6-414a-8ac9-d5e0fbc49dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100, 4])\n",
      "5.825078010559082\n",
      "torch.Size([2, 100, 4])\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "x = torch.rand(2,100,4)\n",
    "print(x.shape)\n",
    "\n",
    "\n",
    "m = SkipGRU(4,4,layer_num=2)\n",
    "tic = time.time()\n",
    "for i in range(100) :\n",
    "    y,_ = m(x)\n",
    "toc = time.time()\n",
    "\n",
    "print(toc - tic)\n",
    "print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03059192-2d99-4786-86b2-583bab35879a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.040289878845214844\n"
     ]
    }
   ],
   "source": [
    "m = nn.GRU(4,4,num_layers =2)\n",
    "tic = time.time()\n",
    "for i in range(100) :\n",
    "    y,_ = m(x)\n",
    "toc = time.time()\n",
    "print(toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9259bf-03fd-4044-b822-92bfd9ea0434",
   "metadata": {},
   "source": [
    "임의 Module 구현은 100배 느리다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902f06f6-b96b-4545-a15f-7eeaebf2c85c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
