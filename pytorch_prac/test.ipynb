{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc7255de-e5bd-4e9b-880b-3931783490c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed843733-40f6-4ccd-b6f4-c480bdc27869",
   "metadata": {},
   "source": [
    "## chunk\n",
    "https://pytorch.org/docs/stable/generated/torch.chunk.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ab2ac63-d777-4106-b2e3-6870dbe3a773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7341, 0.4233],\n",
      "        [0.4520, 0.9506],\n",
      "        [0.4502, 0.4742]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3,2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a2be7747-5968-42e9-9f30-c48696e7c57f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.7341, 0.4233]]), tensor([[0.4520, 0.9506]]), tensor([[0.4502, 0.4742]]))\n"
     ]
    }
   ],
   "source": [
    "y = torch.chunk(x,3)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277800d-8f55-472f-b60e-72436d964dad",
   "metadata": {},
   "source": [
    "## CLSTM\n",
    "Inputs: input, (h_0, c_0)\n",
    "input of shape (seq_len, batch, input_size): tensor containing the features of the input sequence. The input can also be a packed variable length sequence. See torch.nn.utils.rnn.pack_padded_sequence() or torch.nn.utils.rnn.pack_sequence() for details.\n",
    "\n",
    "h_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. If the LSTM is bidirectional, num_directions should be 2, else it should be 1. If proj_size > 0 was specified, the shape has to be (num_layers * num_directions, batch, proj_size).\n",
    "\n",
    "c_0 of shape (num_layers * num_directions, batch, hidden_size): tensor containing the initial cell state for each element in the batch.\n",
    "\n",
    "If (h_0, c_0) is not provided, both h_0 and c_0 default to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a55d2704-74ed-463c-8bc6-084a646903e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 2\n",
    "C = 3\n",
    "F = 4\n",
    "T = 6 \n",
    "\n",
    "rnn_unit = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6d3a73-5759-4ca4-bbc9-ac5fd4aa8d65",
   "metadata": {},
   "source": [
    "input : (B,C,F,T)  \n",
    "LSTM input(batch_first)  : (B,T,C*F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3306c80-7c31-430f-9eff-7ad826d6d83b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'proj_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-74bb6ea36fcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlstm_r\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn_unit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproj_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# batch_first : (T,B,F) -> (B,T,F)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/dnn/lib/python3.8/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'proj_size'"
     ]
    }
   ],
   "source": [
    "x = torch.rand(B,C,F,T,2)\n",
    "lstm_r = nn.LSTM(input_size=F*C,hidden_size = rnn_unit,num_layers = 2, batch_first=True,bidirectional=True,proj_size=0) # batch_first : (T,B,F) -> (B,T,F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a277b359-83c8-4cae-9cd1-a2786b9b985d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 12, 2])\n"
     ]
    }
   ],
   "source": [
    "z = torch.reshape(x,[B,T,C*F,2])\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32d0a456-9471-4c41-9d68-2d28b72a34ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "2\n",
      "torch.Size([2, 6, 32])\n"
     ]
    }
   ],
   "source": [
    "z_r = z[:,:,:,0]\n",
    "out_lstm_r = lstm_r(z_r)\n",
    "print(type(out_lstm_r))\n",
    "print(len(out_lstm_r))\n",
    "print(out_lstm_r[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345fff0-8609-4d6c-8bc0-42cbc6191211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd812d25-0c08-474c-9439-ca9345392dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
